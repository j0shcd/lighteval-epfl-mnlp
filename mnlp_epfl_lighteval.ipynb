{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0shcd/lighteval-epfl-mnlp/blob/main/mnlp_epfl_lighteval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxG6sZKiMT2T",
        "outputId": "79d013fc-184d-4d88-deb4-f11b988a8f90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRUkdWulJSUd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ahuou/lighteval-epfl-mnlp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd lighteval-epfl-mnlp\n",
        "!pip install -e lighteval-epfl-mnlp/[quantization]  # on h100, you can `pip install -e .[quantization,quantization_fbgemm]` to install fbgemm package"
      ],
      "metadata": {
        "id": "IQwyqc-8JoLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers==4.51.3"
      ],
      "metadata": {
        "id": "jF5KUdp5J69k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU38tCfBJ_Sa",
        "outputId": "45aa09c3-0fe7-43c6-d063-6d2a92464f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "The token `MNLP` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `MNLP`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Often default to /home/<user>/.cache/huggingface/hub/\n",
        "!export HF_HOME=\"root/.cache/huggingface/hub/\"\n",
        "# You can find this token in your user profile on HuggingFace Hub\n",
        "# HF_ACCESS_TOKEN="
      ],
      "metadata": {
        "id": "rkatYrLXKKf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lighteval-epfl-mnlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayo46BHALdDE",
        "outputId": "36c5b913-d6de-4f60-ed49-9a4cf98e6295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lighteval-epfl-mnlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lighteval accelerate \\\n",
        "    --eval-mode \"rag\" \\\n",
        "    --save-details \\\n",
        "    --override-batch-size 1 \\\n",
        "    --custom-tasks \"community_tasks/mnlp_mcqa_evals.py\" \\\n",
        "    --output-dir \"../drive/MyDrive/MNLP_output\" \\\n",
        "    model_configs/rag_model.yaml \\\n",
        "    \"community|mnlp_mcqa_evals|0|0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQvO7mCnKQqd",
        "outputId": "4f2bb5de-9bfd-475e-a0cf-a211804f5115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-08 06:50:54,461] [\u001b[32m    INFO\u001b[0m]: NumExpr defaulting to 12 threads. (utils.py:162)\u001b[0m\n",
            "[2025-06-08 06:50:54,703] [\u001b[32m    INFO\u001b[0m]: PyTorch version 2.6.0+cu124 available. (config.py:54)\u001b[0m\n",
            "[2025-06-08 06:50:54,704] [\u001b[32m    INFO\u001b[0m]: Polars version 1.21.0 available. (config.py:66)\u001b[0m\n",
            "[2025-06-08 06:50:54,705] [\u001b[32m    INFO\u001b[0m]: Duckdb version 1.2.2 available. (config.py:77)\u001b[0m\n",
            "[2025-06-08 06:50:54,705] [\u001b[32m    INFO\u001b[0m]: TensorFlow version 2.18.0 available. (config.py:112)\u001b[0m\n",
            "[2025-06-08 06:50:54,707] [\u001b[32m    INFO\u001b[0m]: JAX version 0.5.2 available. (config.py:125)\u001b[0m\n",
            "2025-06-08 06:50:55.302543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-08 06:50:55.320684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749365455.342339    9125 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749365455.348977    9125 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-08 06:50:55.370375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-06-08 06:51:02,730] [\u001b[32m    INFO\u001b[0m]: Loading faiss with AVX512 support. (_loader.py:113)\u001b[0m\n",
            "[2025-06-08 06:51:02,730] [\u001b[32m    INFO\u001b[0m]: Could not load library with AVX512 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\") (_loader.py:118)\u001b[0m\n",
            "[2025-06-08 06:51:02,730] [\u001b[32m    INFO\u001b[0m]: Loading faiss with AVX2 support. (_loader.py:125)\u001b[0m\n",
            "[2025-06-08 06:51:02,730] [\u001b[32m    INFO\u001b[0m]: Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\") (_loader.py:130)\u001b[0m\n",
            "[2025-06-08 06:51:02,730] [\u001b[32m    INFO\u001b[0m]: Loading faiss. (_loader.py:148)\u001b[0m\n",
            "[2025-06-08 06:51:02,751] [\u001b[32m    INFO\u001b[0m]: Successfully loaded faiss. (_loader.py:150)\u001b[0m\n",
            "[2025-06-08 06:51:03,406] [\u001b[32m    INFO\u001b[0m]: You set `multichoice_continuations_start_space` to true. This will force multichoice continuations to use a starting space (transformers_model.py:192)\u001b[0m\n",
            "[2025-06-08 06:51:03,406] [\u001b[32m    INFO\u001b[0m]: You set `multichoice_continuations_start_space` to true. This will force multichoice continuations to use a starting space (transformers_model.py:192)\u001b[0m\n",
            "[2025-06-08 06:51:03,407] [\u001b[32m    INFO\u001b[0m]: Test gather tensor (parallelism.py:133)\u001b[0m\n",
            "[2025-06-08 06:51:03,524] [\u001b[32m    INFO\u001b[0m]: gathered_tensor tensor([0], device='cuda:0'), should be [0] (parallelism.py:136)\u001b[0m\n",
            "[2025-06-08 06:51:03,524] [\u001b[32m    INFO\u001b[0m]: Initializing RAG Model (pipeline.py:181)\u001b[0m\n",
            "[2025-06-08 06:51:03,524] [\u001b[32m    INFO\u001b[0m]: --- LOADING MODEL --- (pipeline.py:216)\u001b[0m\n",
            "[2025-06-08 06:51:04,089] [\u001b[32m    INFO\u001b[0m]: Tokenizer truncation and padding size set to the left side. (embed_model.py:272)\u001b[0m\n",
            "[2025-06-08 06:51:04,344] [\u001b[32m    INFO\u001b[0m]: Load pretrained SentenceTransformer: Stergios-Konstantinidis/MNLP_M3_tokenizer_tuned (SentenceTransformer.py:219)\u001b[0m\n",
            "Loading KB: 100% 100000/100000 [00:04<00:00, 23504.91it/s]\n"
          ]
        }
      ]
    }
  ]
}